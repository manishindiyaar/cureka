# Story: 1.1 - Mobile Voice Agent Integration

**Epic**: Voice Assistant Integration
**Status**: Completed

### Implementation Notes
- Voice service module created with Vapi integration support
- WebSocket service added for real-time event handling
- Voice chat screen implemented with transcript display
- AIButton enhanced with authentication checks and loading states
- Basic test coverage for voice service module

### Debug Log References
- Vapi SDK integration: Uses @vapi-ai/web for browser compatibility
- Mock implementation in place for actual voice input/output until Vapi RN SDK available
- WebSocket connection established for real-time events from backend

### Change Log
- Created: src/services/vapi.service.ts - Voice service module
- Created: src/services/websocket.service.ts - WebSocket client for events
- Modified: app/(chat)/chat.tsx - Redirects to voice-chat
- Created: app/(chat)/voice-chat.tsx - Voice interaction screen
- Modified: src/components/AIButton.tsx - Enhanced for voice support
**Story Type**: Frontend

## Story Statement
As a patient using the mobile app,
I want to tap the "Talk to AI" button and have a real-time voice conversation within the app,
so that I can get help without making a phone call.

## Acceptance Criteria
1. The Vapi client-side SDK (@vapi-ai/web) is successfully installed and configured in the apps/mobile_app/cureka Expo project
2. When the user taps the AI button, the app first makes a secure call to our backend's POST /api/v1/sessions/vapi/start endpoint with JWT auth
3. The app successfully receives the temporary session configuration from our backend
4. The Vapi SDK is initialized using the received configuration data
5. The SDK successfully requests and is granted microphone permissions from the user
6. A real-time, two-way audio stream is established between the app and Vapi's servers
7. The app's UI is updated to clearly indicate the status of the voice session (e.g., "Connecting...", "Listening...", "AI is speaking...", "Session ended")
8. The conversation transcript is displayed in real-time on the screen
9. When the conversation ends, the connection is properly terminated and session is updated
10. Multiple patients can have simultaneous voice sessions without interference

## Technical Context

### Current State
- Backend has secure Vapi session endpoint (POST /api/v1/sessions/vapi/start)
- WebSocket infrastructure is set up for real-time events
- Mobile app has empty chat screen and AI button that navigates to it

### Required Implementation
1. **Voice Service Module** (`services/vapi.service.ts`)
   - Initialize Vapi client with backend configuration
   - Manage microphone permissions
   - Handle session lifecycle events
2. **Chat Screen** (`app/(chat)/voice-chat.tsx`)
   - Replace existing empty chat.tsx
   - Display real-time conversation transcripts
   - Connection status indicators
3. **State Management**
   - Track voice session states (connecting/connected/listening/speaking/disconnected)
   - Manage conversation history
4. **WebSocket Integration**
   - Connect to backend WebSocket for real-time events
   - Receive and display transcripts as voice data is processed

## Dev Notes

### Architecture Alignment
According to current mobile app structure:
- Use existing authentication flow with JWT tokens stored in SecureStore
- Follow React Native + Expo patterns established in the codebase
- Integrate with existing AIButton component (update to support voice)

### Implementation Approach
Based on Vapi SDK documentation and React Native requirements:

```typescript
// Voice Service Example
class VapiVoiceService {
  private vapiClient: VapiClient;
  private sessionId: string;

  async initializeVoiceSession(patientToken: string) {
    // Call backend for config
    const config = await fetch('/api/v1/sessions/vapi/start', {
      headers: { Authorization: `Bearer ${patientToken}` }
    });

    // Initialize Vapi with provided config
    this.vapiClient = new VapiClient(config.data);
    this.sessionId = config.session_id;

    // Setup event handlers
    this.vapiClient.on('transcript', this.handleTranscript);
    this.vapiClient.on('conversation-update', this.handleConversationUpdate);
  }
}
```

### Security Requirements
- JWT authentication for all backend communications
- No direct API keys exposed to client
- Session tokens expire after use

### Performance Requirements
- < 5 seconds connection establishment time
- < 200ms latency for voice interactions
- Support background app state handling

### Testing Requirements
[Source: architecture/testing-strategy.md#unit-testing]
- Unit test voice service module
- Integration test with mock Vapi SDK
- Test microphone permission flows
- Verify session cleanup

### Tasks / Subtasks

1. **Install Vapi Dependencies** [AC: 1] [x]
   - Added @vapi-ai/web, socket.io-client, expo-av, buffer for voice support
   - Add @vapi-ai/web to package.json
   - Install and configure permissions libraries

2. **Create Voice Service** [AC: 2-6] [x]
   - Implement VapiService class for SDK integration
   - Handle authentication and session management
   - Setup event handlers for voice data

3. **Implement Voice Chat Screen** [AC: 7-9]
   - Replace app/(chat)/chat.tsx
   - Create UI for voice interactions
   - Display real-time transcripts
   - Connection status indicators

4. **Update AIButton Component** [AC: 2] [x]
   - Added authentication check before navigation
   - Shows loading state while initializing
   - Updated icon from 'comments' to 'microphone'
   - Add voice session initialization
   - Handle loading states
   - Error handling for connection failures

5. **WebSocket Integration** [AC: 8-10]
   - Connect to backend socket.io
   - Receive real-time transcripts
   - Handle multiple concurrent sessions

6. **Testing Implementation** [AC: Unit tests required]
   - Test voice service module
   - Test permission handling
   - Mock Vapi SDK for testing
   - Integration test scenarios

## Dev Agent Record
**Status**: Not Started
**File**: docs/stories/1.1.Mobile_Voice_Agent_Integration.story.md
**Branch**: feature/1.1-mobile-voice-agent

### Implementation Notes
To be filled during implementation

### Completion Notes
To be filled during completion

### Debug Log References
To be filled during implementation